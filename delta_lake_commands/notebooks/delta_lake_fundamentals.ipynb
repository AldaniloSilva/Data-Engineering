{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Delta Lake basic commands**\n",
    "\n",
    "This notebook aims to show delta lake commands on pyspark.  \n",
    "There are two repository on github that help me and here are the credits:  \n",
    "\n",
    "<a href=\"https://github.com/jaumpedro214/posts/tree/main/delta_lake_fundamentals\">jaumpedro214</a>   \n",
    "<a href= \"https://github.com/handreassa/delta-docker/blob/main/notebooks/example.ipynb\">handreassa</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Spark and import libs\n",
    "\n",
    "import pyspark\n",
    "from delta import *\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"LocalDelta\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the schema\n",
    "\n",
    "SCHEMA = StructType(\n",
    "    [\n",
    "        StructField('id', StringType(), True), \n",
    "        StructField('data_inversa', StringType(), True), \n",
    "        StructField('dia_semana', StringType(), True), \n",
    "        StructField('horario', StringType(), True), \n",
    "        StructField('uf', StringType(), True), \n",
    "        StructField('br', StringType(), True), \n",
    "        StructField('km', StringType(), True), \n",
    "        StructField('municipio', StringType(), True), \n",
    "        StructField('causa_acidente', StringType(), True), \n",
    "        StructField('tipo_acidente', StringType(), True), \n",
    "        StructField('classificacao_acidente', StringType(), True), \n",
    "        StructField('fase_dia', StringType(), True), \n",
    "        StructField('sentido_via', StringType(), True), \n",
    "        StructField('condicao_metereologica', StringType(), True), \n",
    "        StructField('tipo_pista', StringType(), True), \n",
    "        StructField('tracado_via', StringType(), True), \n",
    "        StructField('uso_solo', StringType(), True), \n",
    "        StructField('pessoas', IntegerType(), True), \n",
    "        StructField('mortos', IntegerType(), True), \n",
    "        StructField('feridos_leves', IntegerType(), True), \n",
    "        StructField('feridos_graves', IntegerType(), True), \n",
    "        StructField('ilesos', IntegerType(), True), \n",
    "        StructField('ignorados', IntegerType(), True), \n",
    "        StructField('feridos', IntegerType(), True), \n",
    "        StructField('veiculos', StringType(), True), \n",
    "        StructField('latitude', DoubleType(), True), \n",
    "        StructField('longitude', DoubleType(), True), \n",
    "        StructField('regional', StringType(), True), \n",
    "        StructField('delegacia', StringType(), True), \n",
    "        StructField('uop', StringType(), True)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/24 16:30:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "|    id|data_inversa|  dia_semana| horario| uf| br|   km|           municipio|      causa_acidente|       tipo_acidente|classificacao_acidente| fase_dia|sentido_via|condicao_metereologica|tipo_pista|tracado_via|uso_solo|pessoas|mortos|feridos_leves|feridos_graves|ilesos|ignorados|feridos|veiculos|    latitude|   longitude|regional|delegacia|           uop|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "|260068|  2020-01-01|quarta-feira|05:40:00| PA|316|   84|SAO FRANCISCO DO ...|Falta de Atenção ...|Saída de leito ca...|   Com Vítimas Feridas|Pleno dia|Decrescente|             Céu Claro|   Simples|       Reta|     Não|      2|     0|            2|             0|     0|        0|      2|       1|  -1.3101929|-47.74456398| SPRF-PA| DEL01-PA|UOP02-DEL01-PA|\n",
      "|260073|  2020-01-01|quarta-feira|06:00:00| MG|262|  804|             UBERABA|Falta de Atenção ...| Colisão transversal|   Com Vítimas Feridas|Pleno dia|Decrescente|             Céu Claro|     Dupla|       Reta|     Sim|      4|     0|            1|             0|     3|        0|      1|       2|-19.76747537|-47.98725511| SPRF-MG| DEL13-MG|UOP01-DEL13-MG|\n",
      "|260087|  2020-01-01|quarta-feira|06:00:00| BA|116|  191|             CANUDOS|   Condutor Dormindo|Saída de leito ca...|    Com Vítimas Fatais|Pleno dia|  Crescente|               Nublado|   Simples|       Reta|     Não|      1|     1|            0|             0|     0|        0|      0|       1|-10.32002103|-39.06425211| SPRF-BA| DEL07-BA|UOP02-DEL07-BA|\n",
      "|260116|  2020-01-01|quarta-feira|10:08:00| SP|116|   71|           APARECIDA|Não guardar distâ...|    Colisão traseira|   Com Vítimas Feridas|Pleno dia|  Crescente|                   Sol|     Dupla|       Reta|     Sim|      3|     0|            2|             0|     1|        0|      2|       2|-22.85651665|-45.23114328| SPRF-SP| DEL08-SP|UOP01-DEL08-SP|\n",
      "|260129|  2020-01-01|quarta-feira|12:10:00| MG|262|380,9|             JUATUBA|   Condutor Dormindo|Saída de leito ca...|   Com Vítimas Feridas|Pleno dia|  Crescente|             Céu Claro|     Dupla|      Curva|     Não|      1|     0|            1|             0|     0|        0|      1|       1|  -19.947864|  -44.381226| SPRF-MG| DEL01-MG|UOP03-DEL01-MG|\n",
      "+------+------------+------------+--------+---+---+-----+--------------------+--------------------+--------------------+----------------------+---------+-----------+----------------------+----------+-----------+--------+-------+------+-------------+--------------+------+---------+-------+--------+------------+------------+--------+---------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data from source\n",
    "\n",
    "df_acidentes = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)    \n",
    "    .load(\"./../data/acidentes/datatran2020.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Write a delta table format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "path = \"./../data/delta/acidentes/\"\n",
    "\n",
    "(df_acidentes.write\n",
    "             .format(\"delta\")\n",
    "             .mode(\"overwrite\")\n",
    "             .save(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Read from a delta table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_delta = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+----------+--------+---+\n",
      "|    id|data_inversa|dia_semana| horario| uf|\n",
      "+------+------------+----------+--------+---+\n",
      "|324581|  2020-11-29|   domingo|12:30:00| SC|\n",
      "|324585|  2020-11-29|   domingo|12:30:00| PE|\n",
      "|324586|  2020-11-29|   domingo|12:10:00| RJ|\n",
      "|324587|  2020-11-29|   domingo|13:00:00| SC|\n",
      "|324588|  2020-11-29|   domingo|11:50:00| SC|\n",
      "+------+------------+----------+--------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# And that way it is possible execute query as usual \n",
    "\n",
    "df_acidentes_delta.select([\"id\", \"data_inversa\", \"dia_semana\", \"horario\", \"uf\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.Add new data on the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's append 2019 data to the Delta table. Nothing different if was parquet table format\n",
    "\n",
    "df_acidentes_2019 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(SCHEMA)    \n",
    "    .load(\"./../data/acidentes/datatran2019.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_acidentes_2019.write.format(\"delta\").mode(\"append\").save(path)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131132"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_delta.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. History logs of the table\n",
    "\n",
    "Running the history command it's possible see like a version control history in reverse chronological order.  \n",
    "To read the log, we can use a special python object called `DeltaTable`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|version|           timestamp|userId|userName|operation| operationParameters| job|notebook|clusterId|readVersion|isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "|      7|2023-09-24 16:42:...|  null|    null|    WRITE|{mode -> Append, ...|null|    null|     null|          6|  Serializable|         true|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      6|2023-09-24 16:31:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          5|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      5|2023-09-23 18:08:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          4|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      4|2023-09-23 18:08:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          3|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      3|2023-09-23 16:57:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          2|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      2|2023-09-23 16:56:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          1|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      1|2023-09-23 16:03:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|          0|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "|      0|2023-09-22 22:57:...|  null|    null|    WRITE|{mode -> Overwrit...|null|    null|     null|       null|  Serializable|        false|{numFiles -> 5, n...|        null|Apache-Spark/3.4....|\n",
      "+-------+--------------------+------+--------+---------+--------------------+----+--------+---------+-----------+--------------+-------------+--------------------+------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, path)\n",
    "\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                   |\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|7      |2023-09-24 16:42:12.885|WRITE    |{mode -> Append, partitionBy -> []}   |\n",
      "|6      |2023-09-24 16:31:04.086|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|5      |2023-09-23 18:08:26.662|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|4      |2023-09-23 18:08:22.151|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|3      |2023-09-23 16:57:12.399|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|2      |2023-09-23 16:56:28.117|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|1      |2023-09-23 16:03:00.077|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|0      |2023-09-22 22:57:38.058|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131132"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read a specific version of the table, by default the Spark will read the latest version\n",
    "\n",
    "df_acidentes_latest = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .load(path)\n",
    ")\n",
    "\n",
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# It's possible to read a specific version\n",
    "\n",
    "df_acidentes_version_0 = (\n",
    "    spark\n",
    "    .read.format(\"delta\")\n",
    "    .option(\"versionAsOf\", 0)\n",
    "    .load(path)\n",
    ")\n",
    "\n",
    "df_acidentes_version_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Revert to previous version, this comand save data on the original path where are delta table\n",
    "delta_table.restoreToVersion(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "63576"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now there isn't more 2019 data in the last version\n",
    "# Interesting that don't need read again the data to update value on the variable df_acidentes_latest\n",
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                   |\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "|8      |2023-09-24 17:17:49.697|RESTORE  |{version -> 0, timestamp -> null}     |\n",
      "|7      |2023-09-24 16:42:12.885|WRITE    |{mode -> Append, partitionBy -> []}   |\n",
      "|6      |2023-09-24 16:31:04.086|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|5      |2023-09-23 18:08:26.662|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|4      |2023-09-23 18:08:22.151|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|3      |2023-09-23 16:57:12.399|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|2      |2023-09-23 16:56:28.117|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|1      |2023-09-23 16:03:00.077|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "|0      |2023-09-22 22:57:38.058|WRITE    |{mode -> Overwrite, partitionBy -> []}|\n",
      "+-------+-----------------------+---------+--------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Last operation to revert to version 0 was inclued in the log of history.\n",
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[table_size_after_restore: bigint, num_of_files_after_restore: bigint, num_removed_files: bigint, num_restored_files: bigint, removed_files_size: bigint, restored_files_size: bigint]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually, no information is lost.\n",
    "# We can restore back to the version with the data form 2020 and 2019.\n",
    "delta_table.restoreToVersion(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "131132"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Update\n",
    "The DeltaTable object there is a operation update wutg the SQL syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_inversa|\n",
      "+------------+\n",
      "|    10/06/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "|    01/01/16|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2016 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"./../data/acidentes/datatran2016.csv\")\n",
    ")\n",
    "\n",
    "df_acidentes_2016.select(\"data_inversa\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/24 17:49:35 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 25, schema size: 30\n",
      "CSV file: file:///home/alipio/Data-Engineering/delta_lake_commands/data/acidentes/datatran2016.csv\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_acidentes_2016.write.format(\"delta\").mode(\"append\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "227495"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    UPDATE acidentes_latest\n",
    "    SET data_inversa = CAST( TO_DATE(data_inversa, 'dd/MM/yy') AS STRING)\n",
    "    WHERE data_inversa LIKE '%/16'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.filter(\"data_inversa LIKE '%/16'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------+---------+---------------------------------------------------+\n",
      "|version|timestamp              |operation|operationParameters                                |\n",
      "+-------+-----------------------+---------+---------------------------------------------------+\n",
      "|11     |2023-09-24 18:05:32.664|UPDATE   |{predicate -> [\"EndsWith(data_inversa#5099, /16)\"]}|\n",
      "|10     |2023-09-24 17:49:37.462|WRITE    |{mode -> Append, partitionBy -> []}                |\n",
      "|9      |2023-09-24 17:30:46.723|RESTORE  |{version -> 7, timestamp -> null}                  |\n",
      "|8      |2023-09-24 17:17:49.697|RESTORE  |{version -> 0, timestamp -> null}                  |\n",
      "|7      |2023-09-24 16:42:12.885|WRITE    |{mode -> Append, partitionBy -> []}                |\n",
      "|6      |2023-09-24 16:31:04.086|WRITE    |{mode -> Overwrite, partitionBy -> []}             |\n",
      "|5      |2023-09-23 18:08:26.662|WRITE    |{mode -> Overwrite, partitionBy -> []}             |\n",
      "|4      |2023-09-23 18:08:22.151|WRITE    |{mode -> Overwrite, partitionBy -> []}             |\n",
      "|3      |2023-09-23 16:57:12.399|WRITE    |{mode -> Overwrite, partitionBy -> []}             |\n",
      "|2      |2023-09-23 16:56:28.117|WRITE    |{mode -> Overwrite, partitionBy -> []}             |\n",
      "+-------+-----------------------+---------+---------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delta_table.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96363"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.filter(\"data_inversa LIKE '2016-%'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|data_inversa|\n",
      "+------------+\n",
      "|  2016-03-18|\n",
      "|  2016-03-17|\n",
      "|  2016-03-18|\n",
      "|  2016-03-18|\n",
      "|  2016-03-18|\n",
      "+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.select(\"data_inversa\").filter(\"data_inversa LIKE '2016-%'\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6. Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acidentes_2018 = (\n",
    "    spark\n",
    "    .read.format(\"csv\")\n",
    "    .option(\"delimiter\", \";\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"encoding\", \"ISO-8859-1\")\n",
    "    .schema(SCHEMA)\n",
    "    .load(\"./../data/acidentes/datatran2018.csv\")\n",
    ")\n",
    "\n",
    "# WithColumn here has replace function\n",
    "df_acidentes_2018_zero = df_acidentes_2018.withColumn(\"pessoas\", lit(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|data_inversa|    id|pessoas|\n",
      "+------------+------+-------+\n",
      "|  2018-01-01|100027|      2|\n",
      "|  2018-01-01|100044|      2|\n",
      "|  2018-01-01|100046|      2|\n",
      "|  2018-01-01|100052|      2|\n",
      "|  2018-01-01|100053|      1|\n",
      "+------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2018.filter(\"data_inversa LIKE '2018-%'\").select([\"data_inversa\",\"id\",\"pessoas\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "100023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_acidentes_2018_zero.write.format(\"delta\").mode(\"append\").save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296827"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+\n",
      "|data_inversa|pessoas|\n",
      "+------------+-------+\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "|  2018-01-01|      0|\n",
      "+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter(\"data_inversa LIKE '2018-%'\").select([\"data_inversa\", \"pessoas\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|data_inversa|    id|pessoas|\n",
      "+------------+------+-------+\n",
      "|  2018-01-01|100023|      4|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2018.filter(\"data_inversa LIKE '2018-%' AND id = '100023'\").select([\"data_inversa\",\"id\",\"pessoas\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|data_inversa|    id|pessoas|\n",
      "+------------+------+-------+\n",
      "|  2018-01-01|100023|      0|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_2018_zero.filter(\"data_inversa LIKE '2018-%' AND id = '100023'\").select([\"data_inversa\",\"id\",\"pessoas\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/24 18:58:27 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_acidentes_latest.createOrReplaceTempView(\"acidentes_latest\")\n",
    "df_acidentes_2018.createOrReplaceTempView(\"acidentes_2018_new_counts\")\n",
    "\n",
    "spark.sql(\n",
    "    \"\"\"\n",
    "    MERGE INTO acidentes_latest\n",
    "    USING acidentes_2018_new_counts\n",
    "\n",
    "    ON acidentes_latest.id = acidentes_2018_new_counts.id\n",
    "    AND acidentes_latest.data_inversa = acidentes_2018_new_counts.data_inversa\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET pessoas = acidentes_latest.pessoas + acidentes_2018_new_counts.pessoas\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT *\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|data_inversa|    id|pessoas|\n",
      "+------------+------+-------+\n",
      "|  2018-01-01|100002|      2|\n",
      "|  2018-01-01|100023|      4|\n",
      "|  2018-01-01|100028|      2|\n",
      "|  2018-01-01|100031|      2|\n",
      "|  2018-01-01|100035|      1|\n",
      "+------------+------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter(\"data_inversa LIKE '2018-%'\").select([\"data_inversa\",\"id\",\"pessoas\"]).show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------+-------+\n",
      "|data_inversa|    id|pessoas|\n",
      "+------------+------+-------+\n",
      "|  2018-01-01|100053|      1|\n",
      "+------------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_acidentes_latest.filter(\"data_inversa LIKE '2018-%' AND id = '100054'\").select([\"data_inversa\",\"id\",\"pessoas\"]).show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
